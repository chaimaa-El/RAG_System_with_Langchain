{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install langchain langchain_community langchain-chroma faiss-gpu pymupdf sentence-transformers\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OpeQs9YWz4Q",
        "outputId": "d38054cd-0eb8-4692-e781-7e695c8f4791"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m612.5/612.5 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.7/94.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.7/408.7 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.3/73.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.7/425.7 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.1/164.1 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries from LangChain and other packages\n",
        "\n",
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain , RetrievalQA\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "\n",
        "# Load a PDF document from a specified file path\n",
        "def load_pdf_document(file_path):\n",
        "    loader = PyMuPDFLoader(file_path)\n",
        "    documents = loader.load()\n",
        "    return documents\n",
        "\n",
        "\n",
        "pdf_file_path = \"AI_and_Technology_Knowledge_Base.pdf\"\n",
        "documents = load_pdf_document(pdf_file_path)\n",
        "\n",
        "\n",
        "# Initialize a text splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,  # Size of each text chunk\n",
        "    chunk_overlap=100,  # Overlap between chunks to maintain context\n",
        "    separators=[  # List of separators to determine where to split the text\n",
        "        \"\\n\\n\",\n",
        "        \".\",\n",
        "        \"\\n\",\n",
        "        \" \",\n",
        "        \"\",\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Apply the text splitter to the loaded documents to create chunks of manageable size\n",
        "docs = text_splitter.split_documents(documents)\n",
        "\n",
        "# Function to remove whitespace and newlines from the content of each document\n",
        "def remove_ws(d):\n",
        "    text = d.page_content.replace('\\n','')\n",
        "    d.page_content = text\n",
        "    return d\n",
        "\n",
        "# Apply the whitespace removal function to all documents\n",
        "docs = [remove_ws(d) for d in docs]\n"
      ],
      "metadata": {
        "id": "DfXqWn9-oA1i"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the embedding model using the pre-trained 'all-MiniLM-L6-v2' model\n",
        "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "# This model is optimized for producing high-quality sentence embeddings efficiently.\n",
        "\n",
        "# Create embeddings for the documents using SentenceTransformerEmbeddings\n",
        "embedding = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Use FAISS to create a vector store from the documents and their embeddings\n",
        "vector_store = FAISS.from_documents(docs, embedding)"
      ],
      "metadata": {
        "id": "dn99AgX7noA7"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cache for frequently asked queries\n",
        "cache = defaultdict(str)\n",
        "\n",
        "def retrieve_with_confidence(query, k=3, threshold=0.6):\n",
        "    \"\"\"\n",
        "    Retrieves documents based on the query with a confidence score.\n",
        "\n",
        "    Args:\n",
        "        query (str): The search query for which to retrieve documents.\n",
        "        k (int): The number of top documents to retrieve (default is 3).\n",
        "        threshold (float): The confidence score threshold for filtering documents (default is 0.6).\n",
        "\n",
        "    Returns:\n",
        "        tuple: A list of high confidence documents and a boolean indicating low confidence.\n",
        "    \"\"\"\n",
        "    # Get query embedding\n",
        "    query_embedding = embedding_model.encode([query])\n",
        "\n",
        "    # Retrieve top-k documents using similarity search\n",
        "    retrieved_docs = vector_store.similarity_search(query, k=k)\n",
        "\n",
        "    # Calculate cosine similarity scores\n",
        "    doc_embeddings = [embedding_model.encode(doc.page_content) for doc in retrieved_docs]\n",
        "    scores = cosine_similarity(query_embedding, doc_embeddings)[0]\n",
        "\n",
        "    # Filter documents based on threshold\n",
        "    high_confidence_docs = [doc for doc, score in zip(retrieved_docs, scores) if score >= threshold]\n",
        "    low_confidence = len(high_confidence_docs) == 0  # Check if none of the scores meet the threshold\n",
        "\n",
        "    # Return high confidence documents and low confidence flag\n",
        "    return high_confidence_docs, low_confidence"
      ],
      "metadata": {
        "id": "m0LpjoJcn3BX"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this implementation, I used both the GPT-Neo and GPT-2 models to leverage their unique strengths: GPT-Neo, being a larger model, provides more comprehensive and contextually aware text generation, while GPT-2 offers a lighter, faster alternative for tasks where computational resources or response time are a concern."
      ],
      "metadata": {
        "id": "6bxo1sg8ajkO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the model name for GPT-Neo\n",
        "model_name = \"EleutherAI/gpt-neo-1.3B\"\n",
        "\n",
        "# Load the tokenizer for the GPT-Neo model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Set padding token for models that lack one\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Define the text generation pipeline\n",
        "hf_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=200,\n",
        "    temperature=0.7,\n",
        "    truncation=True,\n",
        "    clean_up_tokenization_spaces=True\n",
        ")\n",
        "llm = HuggingFacePipeline(pipeline=hf_pipeline)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtSIas7KsZUD",
        "outputId": "3dec4c26-24fe-4b6f-c3f3-c2cb791fd8aa"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the model name for GPT 2\n",
        "model_name = \"gpt2\"\n",
        "\n",
        "# Load the tokenizer for the GPT-Neo model\n",
        "tokenizer2 = AutoTokenizer.from_pretrained(model_name)\n",
        "model2 = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Set padding token for models that lack one\n",
        "if tokenizer2.pad_token is None:\n",
        "    tokenizer2.pad_token = tokenizer2.eos_token\n",
        "\n",
        "# Define the text generation pipeline\n",
        "hf_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model2,\n",
        "    tokenizer=tokenizer2,\n",
        "    max_new_tokens=200,\n",
        "    temperature=0.7,\n",
        "    truncation=True,\n",
        "    clean_up_tokenization_spaces=True\n",
        ")\n",
        "llm2 = HuggingFacePipeline(pipeline=hf_pipeline)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ta85MoRGyO4c",
        "outputId": "727f95c7-8cf8-422c-aada-5aabe7e0a38e"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer(query, model):\n",
        "    \"\"\"\n",
        "    Generates an answer to the given query using a retrieval system and an LLM chain based on a language model.\n",
        "\n",
        "    Args:\n",
        "        query (str): The user's question for which an answer is to be generated.\n",
        "        model: The language model used for generating the answer.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated answer based on the context retrieved.\n",
        "    \"\"\"\n",
        "    # Check if the query is already cached to avoid redundant processing\n",
        "    if query in cache:\n",
        "      return cache[query]\n",
        "\n",
        "    # Retrieve documents and confidence level\n",
        "    retrieved_docs, low_confidence = retrieve_with_confidence(query)\n",
        "\n",
        "    if low_confidence:\n",
        "        return \"Could you please clarify your question?\"\n",
        "\n",
        "    # Combine the retrieved documents for context\n",
        "    context = \" \".join([doc.page_content for doc in retrieved_docs])\n",
        "\n",
        "    # Define the prompt template\n",
        "    template = \"\"\"Answer the question based on the given context.\n",
        "\n",
        "                  Context: {context}\n",
        "\n",
        "                  Question: {question}\n",
        "\n",
        "                  Answer: \"\"\"\n",
        "\n",
        "    # Create prompt and LLM chain for the specified model\n",
        "    prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "    llm_chain = LLMChain(prompt=prompt, llm=model)\n",
        "\n",
        "    # Generate the final answer\n",
        "    response = llm_chain.invoke({\"context\": context, \"question\": query})\n",
        "\n",
        "    # Store the response in the cache\n",
        "    cache[query] = response\n",
        "\n",
        "    return response"
      ],
      "metadata": {
        "id": "HNoCWMC1sLA1"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example query\n",
        "query = \"Can you explain AI?\"\n",
        "\n",
        "# Generate an answer using the GPT-Neo model\n",
        "answer_gpt_neo = generate_answer(query, llm)\n",
        "answer_start = answer_gpt_neo['text'].find(\"Answer:\") + len(\"Answer:\")\n",
        "answer = answer_gpt_neo['text'][answer_start:].strip()  # Extracting and stripping whitespace\n",
        "\n",
        "# Print only the answer\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xa38-GptFcQL",
        "outputId": "23c7121e-5785-4afe-8cad-41b7fe8b6daf"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI is defined as a computer system that is able to do a task that would be impossible for a human being.The definition of artificial intelligence (AI) is very broad. AI can be defined as any machine that has the ability to perform a task that is impossible for a human being.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example query\n",
        "query = \"Can you explain AI?\"\n",
        "\n",
        "# Generate an answer using the GPT 2 model\n",
        "answer_gpt2 = generate_answer(query, llm2)\n",
        "answer_start = answer_gpt2['text'].find(\"Answer:\") + len(\"Answer:\")\n",
        "answer = answer_gpt2['text'][answer_start:].strip()  # Extracting and stripping whitespace\n",
        "\n",
        "# Print only the answer\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NaojCP0SG4VM",
        "outputId": "5d3e6bda-ead7-4e44-a273-e0f8956967ae"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI is a combination of two things: human intelligence and machine intelligence. For example, a computer can be programmed todo any number of tasks. One of the most important AI tasks is to learn.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following function, `generate_answer2`, provides an alternative approach to answer generation using the RetrievalQA framework.\n",
        "This setup combines document retrieval with language model generation, retrieving relevant documents and incorporating\n",
        "them into a prompt that guides the language model to generate a response based on the specified context.\n"
      ],
      "metadata": {
        "id": "pRoLnnVJfm4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer2(query, model):\n",
        "    \"\"\"\n",
        "    Generates an answer to the provided query using the RetrievalQA framework.\n",
        "\n",
        "    Args:\n",
        "        query (str): The user's question for which an answer is to be generated.\n",
        "        model: The language model used for generating the answer.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated answer based on the context retrieved.\n",
        "    \"\"\"\n",
        "\n",
        "    # Define the prompt template to structure the input for the LLM\n",
        "    prompt_template = \"\"\"\n",
        "        Context: {context}\n",
        "        Question: {question}\n",
        "        Answer:   \"\"\"\n",
        "    PROMPT = PromptTemplate(\n",
        "        template=prompt_template,\n",
        "        input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "\n",
        "    # Set up the RetrievalQA chain, specifying the language model, chain type, and retriever\n",
        "    retrievalQA = RetrievalQA.from_chain_type(\n",
        "        llm=model,  # The language model to generate answers\n",
        "        chain_type=\"stuff\",  # This indicates that all retrieved documents will be used to formulate the answer\n",
        "        retriever=vector_store.as_retriever(search_kwargs={'k': 3}),  # Retrieves the top 3 relevant documents\n",
        "        return_source_documents=True,  # Optionally return the source documents along with the answer\n",
        "        chain_type_kwargs={\"prompt\": PROMPT}  # Use the defined prompt template\n",
        "    )\n",
        "\n",
        "    # Invoke the RetrievalQA with the provided query to get the result\n",
        "    result = retrievalQA.invoke({\"query\": query})\n",
        "\n",
        "    # Return only the generated answer from the result\n",
        "    return result['result']\n"
      ],
      "metadata": {
        "id": "S_kfQph5Yf6i"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "query = \"Can you explain AI?\"\n",
        "#Generate an answer using the GPT-Neo model\n",
        "result = generate_answer2(query,llm)\n",
        "# Print the result\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpO9pcCS5lKf",
        "outputId": "1963557d-bcfc-4e55-d5fe-db0f6f7778f7"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Context: AI and Technology Knowledge BaseIntroduction to Artificial IntelligenceArtificial Intelligence (AI) is a branch of computer science that focuses on creating machinescapable of performing tasks that would normally require human intelligence. These tasksinclude learning, problem-solving, perception, language understanding, anddecision-making\n",
            "\n",
            ". AI systems can be classified into two broad categories: narrow AI, which isdesigned to perform a narrow task (such as facial recognition or internet searches), andgeneral AI, which has the ability to perform any intellectual task that a human can do. AI istransforming industries, from healthcare and finance to transportation and manufacturing.The evolution of AI can be traced back to early philosophical discussions about the nature ofthe mind and mechanical reasoning\n",
            "\n",
            "Question: Can you explain AI?\n",
            "\n",
            "Answer: \n",
            "\n",
            "In the context of this question, you are probably asking a follow up question about the scope and meaning of AI. The answer may be a bit clearer if you think of AI as a branch of computer science, which I will address in the following paragraphs.\n",
            "\n",
            "AI is a branch of computer science that focuses on creating machinescapable of performing tasks that would normally require human intelligence. These tasksinclude learning, problem-solving, perception, language understanding, anddecision-making.\n",
            "\n",
            "AI is a large subject and complex topic. The field of AI is very broad with many fields of study. The field of AI spans many fields of study, including artificial intelligence, computer science, computer vision, and more.\n",
            "\n",
            "Artificial intelligence, also known as intelligent machine learning, is a branch of computer science that focuses on creating machinescapable of performing tasks that would normally require human intelligence. These tasksinclude learning, problem-solving, perception, language understanding, anddecision\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "query = \"Can you explain AI?\"\n",
        "#Generate an answer using the GPT 2 model\n",
        "result = generate_answer2(query,llm2)\n",
        "# Print the result\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jltmYQ3-bpO3",
        "outputId": "1878e3f3-55c1-4a38-8948-a088baf03797"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Context: AI and Technology Knowledge BaseIntroduction to Artificial IntelligenceArtificial Intelligence (AI) is a branch of computer science that focuses on creating machinescapable of performing tasks that would normally require human intelligence. These tasksinclude learning, problem-solving, perception, language understanding, anddecision-making\n",
            "\n",
            ". AI systems can be classified into two broad categories: narrow AI, which isdesigned to perform a narrow task (such as facial recognition or internet searches), andgeneral AI, which has the ability to perform any intellectual task that a human can do. AI istransforming industries, from healthcare and finance to transportation and manufacturing.The evolution of AI can be traced back to early philosophical discussions about the nature ofthe mind and mechanical reasoning\n",
            "\n",
            "Question: Can you explain AI?\n",
            "\n",
            "Answer: \n",
            "\n",
            ". AI is the technology we use to do things. It is the type of technology that humans are supposed to use to do things.\n",
            "\n",
            ". Artificial intelligence is the system that manages our information systems so that they can perform these tasks.\n",
            "\n",
            ". AI has been used to develop the self-driving cars and other robotic vehicles.\n",
            "\n",
            ". AI has been used to develop a variety of products, such as computers, aircraft, and aircraft surveillance systems.\n",
            "\n",
            ". Artificial intelligence can be used to create new technologies that will change the world.\n",
            "\n",
            ". AI is an important concept in the field of computer science and its applications are well documented.\n",
            "\n",
            ". Artificial Intelligence (AI) is inextricably linked to the development of computing, computing, and information technology\n",
            "\n",
            ". AI technology has been used to create new products, such as computers, aircraft, and aircraft surveillance systems.\n",
            "\n",
            ". AI has been used to create new technologies that will change the world.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "query = \"Can you explain AI?\"\n",
        "#Generate an answer using the GPT 2 model\n",
        "result = generate_answer2(query,llm2)\n",
        "# Print the result\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04EjmYmdbKp5",
        "outputId": "90cc30d9-2175-4bd1-afeb-7de1b6974a36"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Context: AI and Technology Knowledge BaseIntroduction to Artificial IntelligenceArtificial Intelligence (AI) is a branch of computer science that focuses on creating machinescapable of performing tasks that would normally require human intelligence. These tasksinclude learning, problem-solving, perception, language understanding, anddecision-making\n",
            "\n",
            ". AI systems can be classified into two broad categories: narrow AI, which isdesigned to perform a narrow task (such as facial recognition or internet searches), andgeneral AI, which has the ability to perform any intellectual task that a human can do. AI istransforming industries, from healthcare and finance to transportation and manufacturing.The evolution of AI can be traced back to early philosophical discussions about the nature ofthe mind and mechanical reasoning\n",
            "\n",
            "Question: Can you explain AI?\n",
            "\n",
            "Answer: \n",
            "\n",
            "This is an interesting question. First of all, it is important to distinguish the two, because they are fundamentally different. One can use the terms of the two terms to describe many different types of AI, but these terms are all related to the same thing: the ability to do things. For example, while humans are trained in an advanced level of mathematics, the ability to perform complex (or even trivial) tasks would involve an advanced level of intelligence. In contrast, the ability to perform complex tasks would require a limited intellect and a limited knowledge of abstract concepts. In other words, it is not common for humans to be trained in a technical skill which would require a limited intellect and a limited knowledge of abstract concepts.\n",
            "\n",
            "Secondly, one could ask what would be the most important factor in understanding the development of AI. Consider a typical scenario in a large industry. If you look at a large number of products and services, you will see the following:\n",
            "\n",
            "• A large number of\n"
          ]
        }
      ]
    }
  ]
}